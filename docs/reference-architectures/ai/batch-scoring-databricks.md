---
title: Spark-modellek kötegelt kiértékelése az Azure Databricksben
description: Hozzon létre egy méretezhető megoldás, a kötegelt pontozási egy Apache Spark osztályozási modell egy ütemezés szerint az Azure Databricks használatával.
author: njray
ms.date: 02/07/2019
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: azcat-ai
ms.openlocfilehash: cba8d272ddbdbf2c2da94f68b288e9fb79be7de2
ms.sourcegitcommit: 1a3cc91530d56731029ea091db1f15d41ac056af
ms.translationtype: MT
ms.contentlocale: hu-HU
ms.lasthandoff: 04/03/2019
ms.locfileid: "58887811"
---
# <a name="batch-scoring-of-spark-machine-learning-models-on-azure-databricks"></a><span data-ttu-id="dddaf-103">Batch-pontozás a Spark machine learning-modellek az Azure Databricksben</span><span class="sxs-lookup"><span data-stu-id="dddaf-103">Batch scoring of Spark machine learning models on Azure Databricks</span></span>

<span data-ttu-id="dddaf-104">Ez a referenciaarchitektúra bemutatja, hogyan hozhat létre egy méretezhető megoldás, a kötegelt pontozási egy Apache Spark osztályozási modell egy ütemezés szerint az Azure Databricks, egy Azure-ra optimalizált Apache Spark-alapú elemzési platform használatával.</span><span class="sxs-lookup"><span data-stu-id="dddaf-104">This reference architecture shows how to build a scalable solution for batch scoring an Apache Spark classification model on a schedule using Azure Databricks, an Apache Spark-based analytics platform optimized for Azure.</span></span> <span data-ttu-id="dddaf-105">A megoldás, amely más forgatókönyvek is általánosítva sablonként is használható.</span><span class="sxs-lookup"><span data-stu-id="dddaf-105">The solution can be used as a template that can be generalized to other scenarios.</span></span>

<span data-ttu-id="dddaf-106">Az architektúra egy referenciaimplementációt érhető el az [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="dddaf-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Spark-modellek kötegelt kiértékelése az Azure Databricksben](./_images/batch-scoring-spark.png)

<span data-ttu-id="dddaf-108">**A forgatókönyv**: Az eszközintelligencia-(nagy erőforrásigényű) iparági szeretne a költségek és a társított vehesse a műszaki hibáknak váratlan állásidő minimalizálása érdekében.</span><span class="sxs-lookup"><span data-stu-id="dddaf-108">**Scenario**: A business in an asset-heavy industry wants to minimize the costs and downtime associated with unexpected mechanical failures.</span></span> <span data-ttu-id="dddaf-109">IoT-adatok használatával gyűjtött gépeik, létrehozhat egy prediktív karbantartási modellt.</span><span class="sxs-lookup"><span data-stu-id="dddaf-109">Using IoT data collected from their machines, they can create a predictive maintenance model.</span></span> <span data-ttu-id="dddaf-110">Ez a modell lehetővé teszi, hogy az üzleti összetevők karbantartása proaktív módon, és javítsa ki őket, mielőtt meghiúsulnak.</span><span class="sxs-lookup"><span data-stu-id="dddaf-110">This model enables the business to maintain components proactively and repair them before they fail.</span></span> <span data-ttu-id="dddaf-111">A lehető legnagyobb vehesse a műszaki összetevő használja, akkor költségek csökkentését és csökkenthető a leállások.</span><span class="sxs-lookup"><span data-stu-id="dddaf-111">By maximizing mechanical component use, they can control costs and reduce downtime.</span></span>

<span data-ttu-id="dddaf-112">Egy prediktív karbantartási modell gyűjti az adatokat a gépeket, és megőrzi a korábbi példa az összetevők hibáinak.</span><span class="sxs-lookup"><span data-stu-id="dddaf-112">A predictive maintenance model collects data from the machines and retains historical examples of component failures.</span></span> <span data-ttu-id="dddaf-113">A modell majd az összetevők aktuális állapotának figyelésére, és előre jelezni, ha egy adott összetevő a közeljövőben nem használható.</span><span class="sxs-lookup"><span data-stu-id="dddaf-113">The model can then be used to monitor the current state of the components and predict if a given component will fail in the near future.</span></span> <span data-ttu-id="dddaf-114">Gyakori használati esetek és modellezés megközelítéseket, lásd: [Azure AI-útmutató a prediktív karbantartási megoldások][ai-guide].</span><span class="sxs-lookup"><span data-stu-id="dddaf-114">For common use cases and modeling approaches, see [Azure AI guide for predictive maintenance solutions][ai-guide].</span></span>

<span data-ttu-id="dddaf-115">Ez a referenciaarchitektúra lett tervezve, amelyek új adatokat az összetevő gépekről jelenléte esetén.</span><span class="sxs-lookup"><span data-stu-id="dddaf-115">This reference architecture is designed for workloads that are triggered by the presence of new data from the component machines.</span></span> <span data-ttu-id="dddaf-116">Feldolgozás az alábbi lépésekből áll:</span><span class="sxs-lookup"><span data-stu-id="dddaf-116">Processing involves the following steps:</span></span>

1. <span data-ttu-id="dddaf-117">Betöltheti az adatokat, egy Azure Databricks-adattár az alakzatot a külső adatokat az adattárból.</span><span class="sxs-lookup"><span data-stu-id="dddaf-117">Ingest the data from the external data store onto an Azure Databricks data store.</span></span>

2. <span data-ttu-id="dddaf-118">Egy gépi tanulási modell tanítása az adatok átalakítása a tanítási adathalmazt be, akkor jön létre a Spark MLlib modell.</span><span class="sxs-lookup"><span data-stu-id="dddaf-118">Train a machine learning model by transforming the data into a training data set, then building a Spark MLlib model.</span></span> <span data-ttu-id="dddaf-119">MLlib áll a leggyakrabban használt gépi tanulási algoritmusok és segédprogramok optimalizáltuk, hogy kihasználhassák a Spark adatok skálázhatósági képességeket.</span><span class="sxs-lookup"><span data-stu-id="dddaf-119">MLlib consists of most common machine learning algorithms and utilities optimized to take advantage of Spark data scalability capabilities.</span></span>

3. <span data-ttu-id="dddaf-120">A betanított modell előre jelezni a alkalmazni (besorolása) összetevő hibák által az adatok átalakítása egy pontozó adatok csoportba.</span><span class="sxs-lookup"><span data-stu-id="dddaf-120">Apply the trained model to predict (classify) component failures by transforming the data into a scoring data set.</span></span> <span data-ttu-id="dddaf-121">Az adatok Spark MLLib modell pontozása.</span><span class="sxs-lookup"><span data-stu-id="dddaf-121">Score the data with the Spark MLLib model.</span></span>

4. <span data-ttu-id="dddaf-122">Eredmények Store a Databricks adatok áruházban utófeldolgozása használatalapú.</span><span class="sxs-lookup"><span data-stu-id="dddaf-122">Store results on the Databricks data store for post-processing consumption.</span></span>

<span data-ttu-id="dddaf-123">A notebookok biztosított [GitHub] [ github] egyes feladatok végrehajtásához.</span><span class="sxs-lookup"><span data-stu-id="dddaf-123">Notebooks are provided on [GitHub][github] to perform each of these tasks.</span></span>

## <a name="architecture"></a><span data-ttu-id="dddaf-124">Architektúra</span><span class="sxs-lookup"><span data-stu-id="dddaf-124">Architecture</span></span>

<span data-ttu-id="dddaf-125">Az architektúra határozza meg, amely teljes egészében tartalmazza az adatfolyam [Azure Databricks] [ databricks] egy készlete alapján egymás után végrehajtott [notebookok] [ notebooks].</span><span class="sxs-lookup"><span data-stu-id="dddaf-125">The architecture defines a data flow that is entirely contained within [Azure Databricks][databricks] based on a set of sequentially executed [notebooks][notebooks].</span></span> <span data-ttu-id="dddaf-126">Az alábbi összetevőkből áll:</span><span class="sxs-lookup"><span data-stu-id="dddaf-126">It consists of the following components:</span></span>

<span data-ttu-id="dddaf-127">**[Adatfájlok][github]**.</span><span class="sxs-lookup"><span data-stu-id="dddaf-127">**[Data files][github]**.</span></span> <span data-ttu-id="dddaf-128">A referenciaimplementációt használja egy szimulált adatok statikus adatok öt fájl található.</span><span class="sxs-lookup"><span data-stu-id="dddaf-128">The reference implementation uses a simulated data set contained in five static data files.</span></span>

<span data-ttu-id="dddaf-129">**[Adatbetöltési][notebooks]**.</span><span class="sxs-lookup"><span data-stu-id="dddaf-129">**[Ingestion][notebooks]**.</span></span> <span data-ttu-id="dddaf-130">Az adatok betöltési notebook letölti a bemeneti adatok fájlokat Databricks adatkészletek egy gyűjteménybe.</span><span class="sxs-lookup"><span data-stu-id="dddaf-130">The data ingestion notebook downloads the input data files into a collection of Databricks data sets.</span></span> <span data-ttu-id="dddaf-131">IoT-eszközökről származó adatok a való életből vett helyzet adatfolyam lenne-Databricks elérhető storage – például az Azure SQL-kiszolgáló vagy az Azure Blob storage-be.</span><span class="sxs-lookup"><span data-stu-id="dddaf-131">In a real-world scenario, data from IoT devices would stream onto Databricks-accessible storage such as Azure SQL Server or Azure Blob storage.</span></span> <span data-ttu-id="dddaf-132">Databricks támogatja több [adatforrások][data-sources].</span><span class="sxs-lookup"><span data-stu-id="dddaf-132">Databricks supports multiple [data sources][data-sources].</span></span>

<span data-ttu-id="dddaf-133">**Betanítási folyamat**.</span><span class="sxs-lookup"><span data-stu-id="dddaf-133">**Training pipeline**.</span></span> <span data-ttu-id="dddaf-134">Ez a jegyzetfüzet hajtja végre a szolgáltatás mérnöki notebook-analysis adatkészlet létrehozása a feldolgozott adatokból.</span><span class="sxs-lookup"><span data-stu-id="dddaf-134">This notebook executes the feature engineering notebook to create an analysis data set from the ingested data.</span></span> <span data-ttu-id="dddaf-135">Ezután végrehajtja a modell létrehozása jegyzetfüzet, amely a machine learning-modell használatával betanítja a [Apache Spark MLlib] [ mllib] skálázható gépi tanulási kódtár.</span><span class="sxs-lookup"><span data-stu-id="dddaf-135">It then executes a model building notebook that trains the machine learning model using the [Apache Spark MLlib][mllib] scalable machine learning library.</span></span>

<span data-ttu-id="dddaf-136">**Pontozó folyamat**.</span><span class="sxs-lookup"><span data-stu-id="dddaf-136">**Scoring pipeline**.</span></span> <span data-ttu-id="dddaf-137">Ez a jegyzetfüzet hajt végre a szolgáltatás mérnöki notebook pontozó-adatkészlet létrehozása a feldolgozott adatokból, és végrehajtja a pontozási notebookot.</span><span class="sxs-lookup"><span data-stu-id="dddaf-137">This notebook executes the feature engineering notebook to create scoring data set from the ingested data and executes the scoring notebook.</span></span> <span data-ttu-id="dddaf-138">A pontozó notebookot használja a betanított [Spark MLlib] [ mllib-spark] modellt létrehozni az adatokat a megfigyelések a pontozási adatkészletében.</span><span class="sxs-lookup"><span data-stu-id="dddaf-138">The scoring notebook uses the trained [Spark MLlib][mllib-spark] model to generate predictions for the observations in the scoring data set.</span></span> <span data-ttu-id="dddaf-139">Az előrejelzés tárolja az eredményeket tároló, a Databricks-tárolót az új adatkészletet.</span><span class="sxs-lookup"><span data-stu-id="dddaf-139">The predictions are stored in the results store, a new data set on the Databricks data store.</span></span>

<span data-ttu-id="dddaf-140">**A Scheduler**.</span><span class="sxs-lookup"><span data-stu-id="dddaf-140">**Scheduler**.</span></span> <span data-ttu-id="dddaf-141">A Databricks ütemezett [feladat] [ job] leírók kötegelt pontozási a Spark-modell.</span><span class="sxs-lookup"><span data-stu-id="dddaf-141">A scheduled Databricks [job][job] handles batch scoring with the Spark model.</span></span> <span data-ttu-id="dddaf-142">A feladat végrehajtása a pontozási folyamat jegyzetfüzet, a változó argumentumoknak keresztül notebook paramétereket adja meg a hozhat létre, a pontozó adatkészlet és az eredménykészletet adatok tárolására.</span><span class="sxs-lookup"><span data-stu-id="dddaf-142">The job executes the scoring pipeline notebook, passing variable arguments through notebook parameters to specify the details for constructing the scoring data set and where to store the results data set.</span></span>

<span data-ttu-id="dddaf-143">A forgatókönyv egy folyamat folyamatként jön létre.</span><span class="sxs-lookup"><span data-stu-id="dddaf-143">The scenario is constructed as a pipeline flow.</span></span> <span data-ttu-id="dddaf-144">Minden egyes notebook arra optimalizáltuk, hogy hajtsa végre az egyes műveletek egy batch-beállítás: adatfeldolgozást, a funkciófejlesztési, a modell létrehozásának és a modell scorings.</span><span class="sxs-lookup"><span data-stu-id="dddaf-144">Each notebook is optimized to perform in a batch setting for each of the operations: ingestion, feature engineering, model building, and model scorings.</span></span> <span data-ttu-id="dddaf-145">Ehhez a szolgáltatás mérnöki notebook célja bármely, a képzés, a hitelesítési, tesztelési vagy pontozási tevékenységek általános adatkészlet létrehozásához.</span><span class="sxs-lookup"><span data-stu-id="dddaf-145">To accomplish this, the feature engineering notebook is designed to generate a general data set for any of the training, calibration, testing, or scoring operations.</span></span> <span data-ttu-id="dddaf-146">Ebben a forgatókönyvben használjuk egy historikus split stratégia ezeket a műveleteket, így a notebook paraméterek használhatók dátumtartomány szűrés beállítása.</span><span class="sxs-lookup"><span data-stu-id="dddaf-146">In this scenario, we use a temporal split strategy for these operations, so the notebook parameters are used to set date-range filtering.</span></span>

<span data-ttu-id="dddaf-147">A forgatókönyv egy batch-folyamatot hoz létre, mert egy nem kötelező vizsgálat notebookok böngészhet a folyamat jegyzetfüzet kimenetét készletét biztosítunk.</span><span class="sxs-lookup"><span data-stu-id="dddaf-147">Because the scenario creates a batch pipeline, we provide a set of optional examination notebooks to explore the output of the pipeline notebooks.</span></span> <span data-ttu-id="dddaf-148">Ezek a GitHub-adattárban található:</span><span class="sxs-lookup"><span data-stu-id="dddaf-148">You can find these in the GitHub repository:</span></span>

- `1a_raw-data_exploring`
- `2a_feature_exploration`
- `2b_model_testing`
- `3b_model_scoring_evaluation`

## <a name="recommendations"></a><span data-ttu-id="dddaf-149">Javaslatok</span><span class="sxs-lookup"><span data-stu-id="dddaf-149">Recommendations</span></span>

<span data-ttu-id="dddaf-150">Databricks be van állítva, így betölteni és adatelemzésre új adatokkal betanított modellek üzembe helyezése.</span><span class="sxs-lookup"><span data-stu-id="dddaf-150">Databricks is set up so you can load and deploy your trained models to make predictions with new data.</span></span> <span data-ttu-id="dddaf-151">Is használt Databricks ebben a forgatókönyvben, mert a következő további előnyökkel:</span><span class="sxs-lookup"><span data-stu-id="dddaf-151">We used Databricks for this scenario because it provides these additional advantages:</span></span>

- <span data-ttu-id="dddaf-152">Egyszeri bejelentkezésének támogatása az Azure Active Directory hitelesítő adataival.</span><span class="sxs-lookup"><span data-stu-id="dddaf-152">Single sign-on support using Azure Active Directory credentials.</span></span>
- <span data-ttu-id="dddaf-153">Feladatütemező feladatai gyártási folyamatok végrehajtásához.</span><span class="sxs-lookup"><span data-stu-id="dddaf-153">Job scheduler to execute jobs for production pipelines.</span></span>
- <span data-ttu-id="dddaf-154">Teljes mértékben interaktív notebook együttműködéssel, irányítópultok, a REST API-k.</span><span class="sxs-lookup"><span data-stu-id="dddaf-154">Fully interactive notebook with collaboration, dashboards, REST APIs.</span></span>
- <span data-ttu-id="dddaf-155">Korlátlan számú fürtök, amelyek bármilyen méretű méretezhető.</span><span class="sxs-lookup"><span data-stu-id="dddaf-155">Unlimited clusters that can scale to any size.</span></span>
- <span data-ttu-id="dddaf-156">Speciális biztonsági, a szerepköralapú hozzáférés-vezérlés és a naplók.</span><span class="sxs-lookup"><span data-stu-id="dddaf-156">Advanced security, role-based access controls, and audit logs.</span></span>

<span data-ttu-id="dddaf-157">Együttműködik az Azure Databricks szolgáltatást, használja a Databricks [munkaterület] [ workspace] felületet a böngészőben vagy a [parancssori felület] [ cli] (CLI).</span><span class="sxs-lookup"><span data-stu-id="dddaf-157">To interact with the Azure Databricks service, use the Databricks [Workspace][workspace] interface in a web browser or the [command-line interface][cli] (CLI).</span></span> <span data-ttu-id="dddaf-158">A Databricks parancssori felület bármilyen platformon, amely támogatja a Python 2.7.9 3.6 érhető el.</span><span class="sxs-lookup"><span data-stu-id="dddaf-158">Access the Databricks CLI from any platform that supports Python 2.7.9 to 3.6.</span></span>

<span data-ttu-id="dddaf-159">A referenciaimplementáció használ [notebookok] [ notebooks] sorrendben feladatok végrehajtásához.</span><span class="sxs-lookup"><span data-stu-id="dddaf-159">The reference implementation uses [notebooks][notebooks] to execute tasks in sequence.</span></span> <span data-ttu-id="dddaf-160">Minden egyes jegyzetfüzet, a bemeneti adatok ugyanabban az adattárban való köztes adatösszetevők (képzés, tesztelési, pontozási vagy eredmények adatkészletek) tárolja.</span><span class="sxs-lookup"><span data-stu-id="dddaf-160">Each notebook stores intermediate data artifacts (training, test, scoring, or results data sets) to the same data store as the input data.</span></span> <span data-ttu-id="dddaf-161">A célja, hogy könnyen használható, az adott használati esetekhez igény szerint.</span><span class="sxs-lookup"><span data-stu-id="dddaf-161">The goal is to make it easy for you to use it as needed in your particular use case.</span></span> <span data-ttu-id="dddaf-162">A gyakorlatban a jegyzetfüzetek olvasása és írása vissza közvetlenül a storage-bA az Azure Databricks példány az adatforrás kíván csatlakozni.</span><span class="sxs-lookup"><span data-stu-id="dddaf-162">In practice, you would connect your data source to your Azure Databricks instance for the notebooks to read and write directly back into your storage.</span></span>

<span data-ttu-id="dddaf-163">Figyelheti a feladat végrehajtása a Databricks felhasználói felület, az adattár vagy a Databricks [CLI] [ cli] szükség szerint.</span><span class="sxs-lookup"><span data-stu-id="dddaf-163">You can monitor job execution through the Databricks user interface, the data store, or the Databricks [CLI][cli] as necessary.</span></span> <span data-ttu-id="dddaf-164">A fürt használatával figyelheti a [Eseménynapló] [ log] és egyéb [metrikák] [ metrics] Databricks biztosító.</span><span class="sxs-lookup"><span data-stu-id="dddaf-164">Monitor the cluster using the [event log][log] and other [metrics][metrics] that Databricks provides.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="dddaf-165">A teljesítménnyel kapcsolatos megfontolások</span><span class="sxs-lookup"><span data-stu-id="dddaf-165">Performance considerations</span></span>

<span data-ttu-id="dddaf-166">Egy Azure Databricks-fürt automatikus skálázás alapértelmezés szerint lehetővé teszi, hogy futásidőben, Databricks dinamikusan reallocates feldolgozók, hogy figyelembe vegye a feladat jellemzői.</span><span class="sxs-lookup"><span data-stu-id="dddaf-166">An Azure Databricks cluster enables autoscaling by default so that during runtime, Databricks dynamically reallocates workers to account for the characteristics of your job.</span></span> <span data-ttu-id="dddaf-167">Előfordulhat, hogy a folyamat egyes részeit több nagy számítási igényű, mint mások.</span><span class="sxs-lookup"><span data-stu-id="dddaf-167">Certain parts of your pipeline may be more computationally demanding than others.</span></span> <span data-ttu-id="dddaf-168">Databricks hozzáadja a további feldolgozó során ezeket a fázisokat a feladat (és eltávolítja őket, amikor szükség van rájuk már nem).</span><span class="sxs-lookup"><span data-stu-id="dddaf-168">Databricks adds additional workers during these phases of your job (and removes them when they’re no longer needed).</span></span> <span data-ttu-id="dddaf-169">Az automatikus skálázás megkönnyíti a nagy eléréséhez [kihasználtság fürt][cluster], mert nem kell egy munkaterheléshez a fürt üzembe helyezése.</span><span class="sxs-lookup"><span data-stu-id="dddaf-169">Autoscaling makes it easier to achieve high [cluster utilization][cluster], because you don’t need to provision the cluster to match a workload.</span></span>

<span data-ttu-id="dddaf-170">Ezenkívül az összetettebb ütemezett folyamatok használatával fejleszthetők [Azure Data Factory] [ adf] az Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="dddaf-170">Additionally, more complex scheduled pipelines can be developed by using [Azure Data Factory][adf] with Azure Databricks.</span></span>

## <a name="storage-considerations"></a><span data-ttu-id="dddaf-171">A tárterülettel kapcsolatos szempontok</span><span class="sxs-lookup"><span data-stu-id="dddaf-171">Storage considerations</span></span>

<span data-ttu-id="dddaf-172">A referenciaimplementációt, az adatok vannak tárolva közvetlenül az egyszerűség kedvéért egy Databricks-tároló.</span><span class="sxs-lookup"><span data-stu-id="dddaf-172">In this reference implementation, the data is stored directly within Databricks storage for simplicity.</span></span> <span data-ttu-id="dddaf-173">Éles környezetben azonban az adatok tárolhatók a felhőalapú adattárolás például [Azure Blob Storage][blob].</span><span class="sxs-lookup"><span data-stu-id="dddaf-173">In a production setting, however, the data can be stored on cloud data storage such as [Azure Blob Storage][blob].</span></span> <span data-ttu-id="dddaf-174">[Databricks] [ databricks-connect] is támogatja az Azure Data Lake Store, az Azure SQL Data Warehouse, Azure Cosmos DB, az Apache Kafka és Hadoop.</span><span class="sxs-lookup"><span data-stu-id="dddaf-174">[Databricks][databricks-connect] also supports Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka, and Hadoop.</span></span>

## <a name="cost-considerations"></a><span data-ttu-id="dddaf-175">Költségekkel kapcsolatos szempontok</span><span class="sxs-lookup"><span data-stu-id="dddaf-175">Cost considerations</span></span>

<span data-ttu-id="dddaf-176">Az Azure Databricks Spark prémium egy kapcsolódó költségek ajánlattal.</span><span class="sxs-lookup"><span data-stu-id="dddaf-176">Azure Databricks is a premium Spark offering with an associated cost.</span></span> <span data-ttu-id="dddaf-177">Emellett nincsenek standard és prémium szintű Databricks [tarifacsomagok][pricing].</span><span class="sxs-lookup"><span data-stu-id="dddaf-177">In addition, there are standard and premium Databricks [pricing tiers][pricing].</span></span>

<span data-ttu-id="dddaf-178">A jelen esetben a standard díjcsomag is használhatók.</span><span class="sxs-lookup"><span data-stu-id="dddaf-178">For this scenario, the standard pricing tier is sufficient.</span></span> <span data-ttu-id="dddaf-179">Azonban ha az adott alkalmazás automatikus skálázás fürtökhöz nagyobb méretű számítási feladatokat vagy interaktív Databricks irányítópultok kezelésére, a prémium szintű megnövelheti további költségeket.</span><span class="sxs-lookup"><span data-stu-id="dddaf-179">However, if your specific application requires automatically scaling clusters to handle larger workloads or interactive Databricks dashboards, the premium level could increase costs further.</span></span>

<span data-ttu-id="dddaf-180">A megoldás notebookok bármely Spark-alapú platform, távolítsa el a Databricks-specifikus csomagok minimális módosítások is futtathatja.</span><span class="sxs-lookup"><span data-stu-id="dddaf-180">The solution notebooks can run on any Spark-based platform with minimal edits to remove the Databricks-specific packages.</span></span> <span data-ttu-id="dddaf-181">Tekintse meg a következő hasonló megoldásokat az Azure különböző platformokhoz:</span><span class="sxs-lookup"><span data-stu-id="dddaf-181">See the following similar solutions for various Azure platforms:</span></span>

- <span data-ttu-id="dddaf-182">[Az Azure-ban a Python Machine Learning Studióba][python-aml]</span><span class="sxs-lookup"><span data-stu-id="dddaf-182">[Python on Azure Machine Learning Studio][python-aml]</span></span>
- <span data-ttu-id="dddaf-183">[Az SQL Server R services][sql-r]</span><span class="sxs-lookup"><span data-stu-id="dddaf-183">[SQL Server R services][sql-r]</span></span>
- <span data-ttu-id="dddaf-184">[PySpark-az Azure Data Science virtuális gépen][py-dvsm]</span><span class="sxs-lookup"><span data-stu-id="dddaf-184">[PySpark on an Azure Data Science Virtual Machine][py-dvsm]</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="dddaf-185">A megoldás üzembe helyezése</span><span class="sxs-lookup"><span data-stu-id="dddaf-185">Deploy the solution</span></span>

<span data-ttu-id="dddaf-186">Ez a referenciaarchitektúra üzembe helyezéséhez kövesse az ismertetett lépéseket a [GitHub] [ github] tárházat hozhat létre egy méretezhető megoldás, a batch az Azure Databricks Spark modellek pontozási.</span><span class="sxs-lookup"><span data-stu-id="dddaf-186">To deploy this reference architecture, follow the steps described in the [GitHub][github] repository to build a scalable solution for scoring Spark models in batch on Azure Databricks.</span></span>

## <a name="related-architectures"></a><span data-ttu-id="dddaf-187">Kapcsolódó referenciaarchitektúrák</span><span class="sxs-lookup"><span data-stu-id="dddaf-187">Related architectures</span></span>

<span data-ttu-id="dddaf-188">Emellett készítettük el egy referencia-architektúra, amely létrehozásához használja a Spark [valós idejű javaslattételi rendszerek] [ recommendation] az offline, előre kiszámított pontszámokat.</span><span class="sxs-lookup"><span data-stu-id="dddaf-188">We have also built a reference architecture that uses Spark for building [real-time recommendation systems][recommendation] with offline, pre-computed scores.</span></span> <span data-ttu-id="dddaf-189">A javaslattételi rendszerek olyan gyakori forgatókönyvek, ahol pontszámok kötegelt feldolgozásra.</span><span class="sxs-lookup"><span data-stu-id="dddaf-189">These recommendation systems are common scenarios where scores are batch-processed.</span></span>

[adf]: https://azure.microsoft.com/blog/operationalize-azure-databricks-notebooks-using-data-factory/
[ai-guide]: /azure/machine-learning/team-data-science-process/cortana-analytics-playbook-predictive-maintenance
[blob]: https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[cli]: https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html
[cluster]: https://docs.azuredatabricks.net/user-guide/clusters/sizing.html
[databricks]: /azure/azure-databricks/
[databricks-connect]: /azure/azure-databricks/databricks-connect-to-data-sources
[data-sources]: https://docs.databricks.com/spark/latest/data-sources/index.html
[github]: https://github.com/Azure/BatchSparkScoringPredictiveMaintenance
[job]: https://docs.databricks.com/user-guide/jobs.html
[log]: https://docs.databricks.com/user-guide/clusters/event-log.html
[metrics]: https://docs.databricks.com/user-guide/clusters/metrics.html
[mllib]: https://docs.databricks.com/spark/latest/mllib/index.html
[mllib-spark]: https://docs.databricks.com/spark/latest/mllib/index.html#apache-spark-mllib
[notebooks]: https://docs.databricks.com/user-guide/notebooks/index.html
[pricing]: https://azure.microsoft.com/en-us/pricing/details/databricks/
[python-aml]: https://gallery.azure.ai/Notebook/Predictive-Maintenance-Modelling-Guide-Python-Notebook-1
[py-dvsm]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-using-PySpark
[recommendation]: /azure/architecture/reference-architectures/ai/real-time-recommendation
[sql-r]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-Modeling-Guide-using-SQL-R-Services-1
[workspace]: https://docs.databricks.com/user-guide/workspace.html
